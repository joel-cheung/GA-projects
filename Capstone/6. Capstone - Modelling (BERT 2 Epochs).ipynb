{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705a617f",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8dfb1",
   "metadata": {},
   "source": [
    "Twitter is a micro-blogging social media platform with 217.5 million daily active users globally. With 500 million new tweets (posts) daily, the topics of these tweets varies widely – k-pop, politics, financial news… you name it! Individuals use it for news, entertainment, and discussions, while corporations use them to as a marketing tool to reach out to a wide audience. Given the freedom Twitter accords to its user, Twitter can provide a conducive environment for productive discourse, but this freedom can also be abused, manifesting in the forms of racism and sexism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4cfa85",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcfe3f9",
   "metadata": {},
   "source": [
    "With Twitter’s significant income stream coming from advertisers, it is imperative that Twitter keeps a substantial user base. On the other hand, Twitter should maintain a safe space for users and provide some level of checks for the tweets the users put out into the public space, and the first step would be to identify tweets that espouse racist or sexist ideologies, and then Twitter can direct the users to appropriate sources of information where users can learn more about the community that they offend or their subconscious biases so they will be more aware of their racist/sexist tendencies. Thus, to balance, Twitter has to be accurate in filtering inappropriate tweets from innocuous ones, and the kind of inappropriateness of flagged tweets (tag - racist or sexist).\n",
    "\n",
    "F1-scores will be the primary metric as it looks at both precision and recall, each looking at false positives (FPs) and false negatives (FNs) respectively, and is a popular metric for imbalanced data as is the case with the dataset used.\n",
    "\n",
    "For the purpose of explanation, racist tweets are used as the ‘positive’ case.\n",
    "\n",
    "In this context, FPs are the cases where the model erroneously flags out tweets as racist when the tweet is actually innocuous/sexist. FNs are cases where the model erroneously flags out tweets as innocuous/sexist but the tweets are actually racist.\n",
    "\n",
    "Thus, higher F1-scores are preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a89ba68",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95dce205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For NLP data cleaning and preprocessing\n",
    "import re, string, nltk, itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Pickle to save model\n",
    "import pickle\n",
    "\n",
    "# For showing time\n",
    "import time\n",
    "\n",
    "# For NLP Machine Learning processes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "# Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# PyTorch LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Tokenization for LSTM\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Transformers library for BERT\n",
    "import transformers\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, plot_roc_curve, RocCurveDisplay\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bb4ccf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([0.], device='cuda:0')\n",
      "1.12.0\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Checking if CUDA is working as intended\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.zeros(1).cuda())\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39308301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11225201808828796437\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2963419955\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7816740847761862436\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 950M, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7e3229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed for reproducibility\n",
    "import random\n",
    "\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b54d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing display settings\n",
    "pd.set_option('display.max_row', 100)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aadea0",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff682b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = pd.read_csv('../Capstone/data/twitter_char_4_gram_lemm_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b0802e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Annotation', 'Text_lemm_char_4_gram'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "575f2120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotation</th>\n",
       "      <th>Text_lemm_char_4_gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>read cont onte ntex text extn xtno chan hang ange mean eani anin ning hist isto stor tory isla slam lami amic slav lave aver very</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>idio diot clai laim peop eopl ople stop beco ecom come terr erro rror rori oris rist make terr erro rror rori oris rist isla slam lami amic mica ical call ally brai rain dead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>call sexi exis xist auto plac lace woul ould rath athe ther talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>wron rong foll ollo llow exam xamp ampl mple moha oham hamm amme mmed qura uran exac xact actl ctly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>saud audi prea reac each ache cher tort ortu rtur ture five year earo arol rold daug augh ught ghte hter deat eath rele elea leas ease</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Annotation  \\\n",
       "0           0   \n",
       "1           0   \n",
       "2           1   \n",
       "3           2   \n",
       "4           0   \n",
       "\n",
       "                                                                                                                                                            Text_lemm_char_4_gram  \n",
       "0                                               read cont onte ntex text extn xtno chan hang ange mean eani anin ning hist isto stor tory isla slam lami amic slav lave aver very  \n",
       "1  idio diot clai laim peop eopl ople stop beco ecom come terr erro rror rori oris rist make terr erro rror rori oris rist isla slam lami amic mica ical call ally brai rain dead  \n",
       "2                                                                                                                call sexi exis xist auto plac lace woul ould rath athe ther talk  \n",
       "3                                                                             wron rong foll ollo llow exam xamp ampl mple moha oham hamm amme mmed qura uran exac xact actl ctly  \n",
       "4                                          saud audi prea reac each ache cher tort ortu rtur ture five year earo arol rold daug augh ught ghte hter deat eath rele elea leas ease  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "199d056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = ['none', 'sexism', 'racism']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631082f8",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "053c1ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the creating the X and Y columns for the character 4-gram based on lemmatized text dataset\n",
    "X, y = twitter['Text_lemm_char_4_gram'].values, twitter['Annotation'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5259cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducting train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06e84a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee056619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling to fit majority class (none)\n",
    "ros = RandomOverSampler()\n",
    "X_train_os, y_train_os = ros.fit_resample(np.array(X_train).reshape(-1,1),np.array(y_train).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3fcb1",
   "metadata": {},
   "source": [
    "### BERT-specific Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2c1315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_os = X_train_os.flatten()\n",
    "y_train_os = y_train_os.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51b2f5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 7913],\n",
       "       [   1, 7913],\n",
       "       [   2, 7913]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y_train_os, return_counts=True)\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2ee356",
   "metadata": {},
   "source": [
    "Since we need to tokenize the tweets (get \"input ids\" and \"attention masks\") for BERT, we load the specific BERT tokenizer from the Hugging Face library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8778c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de81589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a custom tokenizer function using the loaded tokenizer.\n",
    "def bert_tokenizer(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]` special tokens\n",
    "            max_length=MAX_LEN,             # Choose max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length \n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc578cdf",
   "metadata": {},
   "source": [
    "Since we need to specify the length of the longest tokenized sentence, we tokenize the train tweets using the \"encode\" method of the original BERT tokenizer and check the longest sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac9bf8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  183\n"
     ]
    }
   ],
   "source": [
    "# Tokenize train tweets\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in X_train]\n",
    "\n",
    "# Find the longest tokenized tweet\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9300996d",
   "metadata": {},
   "source": [
    "Choosing the max length as ... (slightly more than the max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e166389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max_len + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44731b3f",
   "metadata": {},
   "source": [
    "Then we can tokenize the train, validation and test tweets using the custom define tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31c78f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "D:\\Anaconda\\envs\\Python\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_masks = bert_tokenizer(X_train_os)\n",
    "val_inputs, val_masks = bert_tokenizer(X_valid)\n",
    "test_inputs, test_masks = bert_tokenizer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a25a1",
   "metadata": {},
   "source": [
    "### Data preprocessing for PyTorch BERT model\n",
    "\n",
    "Since we are using the BERT model built on PyTorch, we need to convert the arrays to pytorch tensors and create dataloaders for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "def39900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target columns to pytorch tensors format\n",
    "train_labels = torch.from_numpy(y_train_os)\n",
    "val_labels = torch.from_numpy(y_valid)\n",
    "test_labels = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d0b0f",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73983768",
   "metadata": {},
   "source": [
    "According to the author of this code (from Kaggle), it is mentioned that the original author of the BERT Model suggests a batch_size of 16 or 32. Since there is a memory allocation issue when using batch_size = 32, I have reduced it to 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "636b3e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d95fa603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df252a",
   "metadata": {},
   "source": [
    "# BERT Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c81bfc",
   "metadata": {},
   "source": [
    "Now we can create a custom BERT classifier class, including the original BERT model (made of transformer layers) and additional Dense layers to perform the desired classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c6dae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class Bert_Classifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(Bert_Classifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of the classifier, and number of labels\n",
    "        n_input = 768\n",
    "        n_hidden = 50\n",
    "        # 3 n_output because there are 3 categories of tweets ('none': 0, 'sexism': 1, 'racism': 2)\n",
    "        n_output = 3\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Add dense layers to perform the classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(n_input,  n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output)\n",
    "        )\n",
    "        # Add possibility to freeze the BERT model\n",
    "        # to avoid fine tuning BERT params (usually leads to worse results)\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Feed input data to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb597a85",
   "metadata": {},
   "source": [
    "Moreover, since we want to define a learning rate scheduler, we define a custom \"initalize_model\" function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6149392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epochs=4):\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = Bert_Classifier(freeze_bert=False)\n",
    "    \n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # learning rate, set to default value\n",
    "                      eps=1e-8    # decay, set to default value\n",
    "                      )\n",
    "    \n",
    "    ### Set up learning rate scheduler ###\n",
    "\n",
    "    # Calculate total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Defint the scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b7c067",
   "metadata": {},
   "source": [
    "We also specify the use of GPU if present (highly recommended for the fine tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80122e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5927a47e",
   "metadata": {},
   "source": [
    "And then we intialize the BERT model calling the \"initialize_model\" function we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b69e355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "D:\\Anaconda\\envs\\Python\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e9f8a",
   "metadata": {},
   "source": [
    "# BERT Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427f8e1a",
   "metadata": {},
   "source": [
    "After defining the custom BERT classifier model, we are ready to start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d382a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Cross entropy Loss function for the multiclass classification task\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def bert_train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        print(\"-\"*10)\n",
    "        print(\"Epoch : {}\".format(epoch_i+1))\n",
    "        print(\"-\"*10)\n",
    "        print(\"-\"*38)\n",
    "        print(f\"{'BATCH NO.':^7} | {'TRAIN LOSS':^12} | {'ELAPSED (s)':^9}\")\n",
    "        print(\"-\"*38)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        \n",
    "        ###TRAINING###\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            \n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass and get logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update model parameters:\n",
    "            # fine tune BERT params and train additional dense layers\n",
    "            optimizer.step()\n",
    "            # update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 100 batches\n",
    "            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                \n",
    "                print(f\"{step:^9} | {batch_loss / batch_counts:^12.6f} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        ###EVALUATION###\n",
    "        \n",
    "        # Put the model into the evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Define empty lists to host accuracy and validation for each batch\n",
    "        val_accuracy = []\n",
    "        val_loss = []\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            batch_input_ids, batch_attention_mask, batch_labels = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            # We do not want to update the params during the evaluation,\n",
    "            # So we specify that we dont want to compute the gradients of the tensors\n",
    "            # by calling the torch.no_grad() method\n",
    "            with torch.no_grad():\n",
    "                logits = model(batch_input_ids, batch_attention_mask)\n",
    "\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            # Get the predictions starting from the logits (get index of highest logit)\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "            # Calculate the validation accuracy \n",
    "            accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
    "            val_accuracy.append(accuracy)\n",
    "\n",
    "        # Compute the average accuracy and loss over the validation set\n",
    "        val_loss = np.mean(val_loss)\n",
    "        val_accuracy = np.mean(val_accuracy)\n",
    "        \n",
    "        # Print performance over the entire training data\n",
    "        time_elapsed = time.time() - t0_epoch\n",
    "        print(\"-\"*61)\n",
    "        print(f\"{'AVG TRAIN LOSS':^12} | {'VAL LOSS':^10} | {'VAL ACCURACY (%)':^9} | {'ELAPSED (s)':^9}\")\n",
    "        print(\"-\"*61)\n",
    "        print(f\"{avg_train_loss:^14.6f} | {val_loss:^10.6f} | {val_accuracy:^17.2f} | {time_elapsed:^9.2f}\")\n",
    "        print(\"-\"*61)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57e9c69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "----------\n",
      "Epoch : 1\n",
      "----------\n",
      "--------------------------------------\n",
      "BATCH NO. |  TRAIN LOSS  | ELAPSED (s)\n",
      "--------------------------------------\n",
      "   100    |   0.951693   |  156.10  \n",
      "   200    |   0.700352   |  171.32  \n",
      "   300    |   0.673859   |  174.32  \n",
      "   400    |   0.707796   |  177.15  \n",
      "   500    |   0.644581   |  207.02  \n",
      "   600    |   0.637614   |  201.56  \n",
      "   700    |   0.649830   |  201.67  \n",
      "   800    |   0.613195   |  201.53  \n",
      "   900    |   0.589683   |  201.54  \n",
      "  1000    |   0.607463   |  201.53  \n",
      "  1100    |   0.580707   |  201.57  \n",
      "  1200    |   0.587274   |  201.69  \n",
      "  1300    |   0.564009   |  201.68  \n",
      "  1400    |   0.527551   |  221.02  \n",
      "  1500    |   0.542023   |  204.62  \n",
      "  1600    |   0.542034   |  201.74  \n",
      "  1700    |   0.547231   |  201.63  \n",
      "  1800    |   0.525580   |  201.51  \n",
      "  1900    |   0.571009   |  201.65  \n",
      "  2000    |   0.525628   |  201.53  \n",
      "  2100    |   0.519836   |  201.46  \n",
      "  2200    |   0.478493   |  201.65  \n",
      "  2300    |   0.436736   |  201.58  \n",
      "  2400    |   0.484396   |  201.62  \n",
      "  2500    |   0.456962   |  211.17  \n",
      "  2600    |   0.454490   |  203.50  \n",
      "  2700    |   0.473160   |  201.55  \n",
      "  2800    |   0.471405   |  201.76  \n",
      "  2900    |   0.433016   |  201.44  \n",
      "  3000    |   0.452953   |  201.55  \n",
      "  3100    |   0.454174   |  201.61  \n",
      "  3200    |   0.341600   |  201.33  \n",
      "  3300    |   0.491878   |  193.11  \n",
      "  3391    |   0.378710   |  173.27  \n",
      "-------------------------------------------------------------\n",
      "AVG TRAIN LOSS |  VAL LOSS  | VAL ACCURACY (%) | ELAPSED (s)\n",
      "-------------------------------------------------------------\n",
      "   0.548124    |  0.746198  |       75.74       |  6820.12 \n",
      "-------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------\n",
      "Epoch : 2\n",
      "----------\n",
      "--------------------------------------\n",
      "BATCH NO. |  TRAIN LOSS  | ELAPSED (s)\n",
      "--------------------------------------\n",
      "   100    |   0.350161   |  194.26  \n",
      "   200    |   0.402606   |  182.81  \n",
      "   300    |   0.360483   |  182.37  \n",
      "   400    |   0.389628   |  182.38  \n",
      "   500    |   0.326090   |  182.42  \n",
      "   600    |   0.384620   |  182.39  \n",
      "   700    |   0.389505   |  182.42  \n",
      "   800    |   0.343706   |  182.47  \n",
      "   900    |   0.338054   |  182.47  \n",
      "  1000    |   0.359315   |  182.37  \n",
      "  1100    |   0.335517   |  182.37  \n",
      "  1200    |   0.360925   |  182.39  \n",
      "  1300    |   0.363423   |  182.36  \n",
      "  1400    |   0.294241   |  182.40  \n",
      "  1500    |   0.368193   |  182.37  \n",
      "  1600    |   0.333218   |  178.52  \n",
      "  1700    |   0.297173   |  178.49  \n",
      "  1800    |   0.338018   |  178.51  \n",
      "  1900    |   0.331371   |  178.44  \n",
      "  2000    |   0.312173   |  178.44  \n",
      "  2100    |   0.284886   |  178.46  \n",
      "  2200    |   0.307563   |  178.47  \n",
      "  2300    |   0.350976   |  178.48  \n",
      "  2400    |   0.317373   |  178.46  \n",
      "  2500    |   0.276527   |  178.40  \n",
      "  2600    |   0.266683   |  178.40  \n",
      "  2700    |   0.305976   |  178.43  \n",
      "  2800    |   0.251906   |  178.43  \n",
      "  2900    |   0.323911   |  178.55  \n",
      "  3000    |   0.266439   |  178.40  \n",
      "  3100    |   0.250026   |  178.47  \n",
      "  3200    |   0.289666   |  178.46  \n",
      "  3300    |   0.300814   |  178.46  \n",
      "  3391    |   0.280220   |  162.08  \n",
      "-------------------------------------------------------------\n",
      "AVG TRAIN LOSS |  VAL LOSS  | VAL ACCURACY (%) | ELAPSED (s)\n",
      "-------------------------------------------------------------\n",
      "   0.325167    |  0.764400  |       80.40       |  6215.28 \n",
      "-------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "bert_train(bert_classifier, train_dataloader, val_dataloader, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ddaf22",
   "metadata": {},
   "source": [
    "# BERT Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b874cb06",
   "metadata": {},
   "source": [
    "Now we define a function similar to the model \"evaluation\", where we feed to the model the test data instead of the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff45c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_predict(model, test_dataloader):\n",
    "    \n",
    "    # Define empty list to host the predictions\n",
    "    preds_list = []\n",
    "    \n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        batch_input_ids, batch_attention_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "        \n",
    "        # Avoid gradient calculation of tensors by using \"no_grad()\" method\n",
    "        with torch.no_grad():\n",
    "            logit = model(batch_input_ids, batch_attention_mask)\n",
    "        \n",
    "        # Get index of highest logit\n",
    "        pred = torch.argmax(logit,dim=1).cpu().numpy()\n",
    "        # Append predicted class to list\n",
    "        preds_list.extend(pred)\n",
    "\n",
    "    return preds_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222acdb4",
   "metadata": {},
   "source": [
    "Then we can call the defined function and get the class predictions of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25705e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preds = bert_predict(bert_classifier, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c4fca",
   "metadata": {},
   "source": [
    "# Results from BERT Classifer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b604e0",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1baa3f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for BERT (2 Epochs) :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        none       0.87      0.82      0.84      2198\n",
      "      sexism       0.65      0.67      0.66       668\n",
      "      racism       0.66      0.84      0.74       394\n",
      "\n",
      "    accuracy                           0.79      3260\n",
      "   macro avg       0.72      0.77      0.75      3260\n",
      "weighted avg       0.80      0.79      0.79      3260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report for BERT (2 Epochs) :\\n', classification_report(y_test, bert_preds, target_names=flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193d17e",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c3f7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_true = y_test, y_pred = bert_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5062a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "\n",
    "ax.set_title('BERT Model (2 Epochs) Confusion Matrix\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['None','Sexism','Racism'])\n",
    "ax.yaxis.set_ticklabels(['None','Sexism','Racism'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0784830a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Bert_Classifier"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the model variable\n",
    "type(bert_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d5c60",
   "metadata": {},
   "source": [
    "# Saving BERT Model with 2 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c21508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "pickle.dump(bert_classifier, open(\"../Capstone/bert_classifier_2epochs.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
